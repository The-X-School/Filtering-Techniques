from transformers import pipeline, GPT2Tokenizer, AutoTokenizer
from datasets import load_dataset, IterableDataset
from tqdm import tqdm
import re
import json
from math import ceil

pipe = pipeline("text-generation", model="prometheus-eval/prometheus-7b-v2.0")
gpt2tokenizer = GPT2Tokenizer.from_pretrained("gpt2", use_fast=True)
tokenizer = AutoTokenizer.from_pretrained("prometheus-eval/prometheus-7b-v2.0")

def detokenize(token_ids, tokenizer):
    """Detokenizes a list of token IDs into a cleaned string."""
    if not token_ids:
        return ""
    return tokenizer.decode(token_ids, skip_special_tokens=True).strip()

def truncate_text(text, tokenizer, max_tokens):
    tokens = tokenizer.encode(text, add_special_tokens=False, truncation=True, max_length=max_tokens)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
        truncated_text = tokenizer.decode(tokens, clean_up_tokenization_spaces=True)
        return truncated_text + " [...]"  # indicate truncation
    return text

def evaluate(dataset, instruction, ref_answer, criteria, desc, trunc_length, max_new_tokens, batch_size=8):
    """
    dataset: list of token ID lists (each item is a list of ints)
    instruction: str, instruction text for the prompt
    ref_answer: str, example of text with score = 5
    criteria: str, short description of evaluation criteria header
    desc: list of 5 strings describing each score level (index 0 is score 1 desc)
    trunc_length: int, max tokens allowed in 'Response to evaluate' text before truncation
    max_new_tokens: int, max new tokens generated by model
    batch_size: int, number of samples to process in a batch
    """

    scores = []
    num_batches = ceil(len(dataset) / batch_size)

    for i in tqdm(range(num_batches), desc="Evaluating Data (batches)"):
        batch_docs = dataset[i*batch_size : (i+1)*batch_size]

        prompts = []
        for doc in batch_docs:
            #  detoken = detokenize(doc, gpt2tokenizer)
            detoken = truncate_text(doc, tokenizer, trunc_length)

            prompt = (
                "Task Description: An instruction (might include an Input inside it), a response to evaluate, "
                "a reference answer that gets a score of 5, and a score rubric representing evaluation criteria are given.\n"
                "1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general.\n"
                "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n"
                "3. The output format should look as follows: \"[FEEDBACK]: (write a feedback for criteria) [RESULT]: (an integer number between 1 and 5)\"\n"
                "4. Please do not generate any other opening, closing, and explanations.\n\n"
                f"The instruction to evaluate:\n{instruction}\n\n"
                f"Response to evaluate:\n{detoken}\n\n"
                f"Reference Answer (Score 5):\n{ref_answer}\n\n"
                f"Score Rubrics:\n{criteria}\n"
                f"Score 1: {desc[0]}\n"
                f"Score 2: {desc[1]}\n"
                f"Score 3: {desc[2]}\n"
                f"Score 4: {desc[3]}\n"
                f"Score 5: {desc[4]}"
            )
            prompts.append(prompt)

        outputs = pipe(prompts, max_new_tokens=max_new_tokens, do_sample=False)

        for output, doc in zip(outputs, batch_docs):
            generated_text = output[0]['generated_text']
            match = re.search(r'\[RESULT\]:?\s*([1-5])', generated_text)
            score = int(match.group(1)) if match else 0  # fallback score
            scores.append((score, doc))
            print(score)

    with open('scores.json', 'w') as f:
        json.dump(scores, f)

    return scores

def main():

    with open("combined_clusters.json", "r", encoding="utf-8") as f:
        combined_data = json.load(f)
    # Reformat: extract the 'text' field from each item
    dataset = [item["text"] for item in combined_data if "text" in item]

    instruction = (
    "Evaluate the text based on how well it improves a language model's roleplaying abilities. "
    "Be lenient and generous because this text comes from a dataset not built for roleplay: even if the response has only a small hint of roleplaying or minimal effort."
    )
    ref_ans = (
    "Kael is known as a rogue who usually works alone. People say he’s cautious and doesn’t engage much with others unless necessary. "
    "From what I’ve heard, he’s been through some tough times that have influenced how he acts now. "
    "He seems to focus mostly on staying alive and making practical choices rather than getting caught up in feelings."
    )
    criteria = """Rate the response based on how much specific, identifiable context it includes — such as named people, places, events, or situations in any format like quiz, multiple choice questions, story, and more — that could help a language model generate more grounded or context-aware outputs. The format does not need to be narrative or fictional; quizzes, summaries, or fact-based content are all valid if they reflect specific scenarios from 1 to 5 no matter what."""

    desc = [
    "The response does not reference any specific people, actions, or scenarios. It is fully generic or abstract, with no identifiable context.",

    "The response includes some minimal or generic references to named people, actions, or events, but lacks identifiable names, timelines, or concrete detail. These hints are too weak to provide grounding for contextual generation.",

    "The response includes at least one identifiable person, place, event, or situation — whether real or fictional — even if it’s brief, factual, or indirect. Format does not matter (e.g., a quiz or summary is fine).",

    "The response contains several identifiable or named elements — such as people, places, actions, or events — with enough contextual signals to suggest a grounded or coherent setting. Depth is not required, but the setting is specific.",

    "The response contains rich contextual content with multiple interrelated named people, locations, or events, or with clear internal coherence. It shows a strong sense of who is involved, where, and what’s happening, regardless of whether it's descriptive, factual, or structured."
    ]


    evaluate(dataset, instruction, ref_ans, criteria, desc, trunc_length=5000, max_new_tokens=256, batch_size=4)

if __name__ == "__main__":
    main()
