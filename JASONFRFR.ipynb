{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (only needed once per session)\n",
    "!pip install -q datasets huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "# Install dependencies (only needed once per session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔐 Paste your token between the quotes (keep it private!)\n",
    "HF_TOKEN = \"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\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "ds = load_dataset(\"OptimalScale/ClimbLab\", split=\"train\", streaming=True)\n",
    "\n",
    "# Toxic-BERT Content Safety Filter for ClimbLab Dataset\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading toxic-bert model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\unosa\\.cache\\huggingface\\hub\\models--unitary--toxic-bert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Toxic-BERT model loaded successfully!\n",
      "💻 Using CPU (slower but works)\n",
      "\n",
      "============================================================\n",
      "🚀 READY TO FILTER YOUR CLIMBLAB DATASET!\n",
      "============================================================\n",
      "\n",
      "⚙️  CURRENT SETTINGS:\n",
      "   📊 Samples to process: 500\n",
      "   🎯 Toxicity threshold: 0.7\n",
      "   📁 Dataset variable: ds\n",
      "\n",
      "🚀 STARTING FILTERING...\n",
      "🔍 Starting toxicity filtering with threshold: 0.7\n",
      "🎯 Threshold explanation: 0.7 means 70% confidence required to mark as toxic\n",
      "📊 Processing samples...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 221\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 STARTING FILTERING...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Start filtering with your dataset\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m safe_samples, toxic_samples, filtering_stats \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_dataset_with_toxic_bert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                           \u001b[49m\u001b[38;5;66;43;03m# Your dataset variable\u001b[39;49;00m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoxicity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOXICITY_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_SAMPLES_TO_PROCESS\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Print detailed results\u001b[39;00m\n\u001b[0;32m    228\u001b[0m print_filtering_results(safe_samples, toxic_samples, filtering_stats)\n",
      "Cell \u001b[1;32mIn[3], line 108\u001b[0m, in \u001b[0;36mfilter_dataset_with_toxic_bert\u001b[1;34m(dataset_stream, toxicity_threshold, max_samples)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Processing samples...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset_stream):\n\u001b[0;32m    109\u001b[0m     text \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# Skip completely empty samples\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\datasets\\iterable_dataset.py:1388\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1385\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m   1386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m ex_iterable:\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m   1390\u001b[0m         \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m         \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m         example \u001b[38;5;241m=\u001b[39m _apply_feature_types_on_example(\n\u001b[0;32m   1393\u001b[0m             example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id\n\u001b[0;32m   1394\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\datasets\\iterable_dataset.py:282\u001b[0m, in \u001b[0;36mArrowExamplesIterable.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    281\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m PythonFormatter()\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, pa_table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_tables_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs):\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pa_subtable \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mto_reader(max_chunksize\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mARROW_READER_BATCH_SIZE_IN_DATASET_ITER):\n\u001b[0;32m    284\u001b[0m             formatted_batch \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat_batch(pa_subtable)\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:89\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m     87\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mor\u001b[39;00m parquet_file\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mrow_group(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, record_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m     90\u001b[0m         parquet_file\u001b[38;5;241m.\u001b[39miter_batches(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     91\u001b[0m     ):\n\u001b[0;32m     92\u001b[0m         pa_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches([record_batch])\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;66;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;66;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;66;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pyarrow\\_parquet.pyx:1638\u001b[0m, in \u001b[0;36miter_batches\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\pyarrow\\error.pxi:89\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:342\u001b[0m, in \u001b[0;36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m         out \u001b[38;5;241m=\u001b[39m read(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ClientError, \u001b[38;5;167;01mTimeoutError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\huggingface_hub\\hf_file_system.py:1015\u001b[0m, in \u001b[0;36mHfFileSystemFile.read\u001b[1;34m(self, length)\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m-> 1015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\fsspec\\spec.py:1790\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[1;34m(self, length)\u001b[0m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1788\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1790\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\fsspec\\caching.py:156\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[1;34m(self, start, end)\u001b[0m\n\u001b[0;32m    154\u001b[0m     part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\huggingface_hub\\hf_file_system.py:969\u001b[0m, in \u001b[0;36mHfFileSystemFile._fetch_range\u001b[1;34m(self, start, end)\u001b[0m\n\u001b[0;32m    958\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39m_build_hf_headers(),\n\u001b[0;32m    961\u001b[0m }\n\u001b[0;32m    962\u001b[0m url \u001b[38;5;241m=\u001b[39m hf_hub_url(\n\u001b[0;32m    963\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_id,\n\u001b[0;32m    964\u001b[0m     revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    967\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[0;32m    968\u001b[0m )\n\u001b[1;32m--> 969\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m504\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\requests\\sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    266\u001b[0m         req,\n\u001b[0;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[0;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\urllib3\\response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\urllib3\\response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\unosa\\anaconda3\\envs\\Pytorch\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"🔄 Loading toxic-bert model...\")\n",
    "\n",
    "# Initialize toxic-bert classifier\n",
    "try:\n",
    "    toxicity_classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"unitary/toxic-bert\",\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    print(\"✅ Toxic-BERT model loaded successfully!\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"🚀 Using GPU acceleration\")\n",
    "    else:\n",
    "        print(\"💻 Using CPU (slower but works)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"📦 Make sure to install: pip install transformers torch\")\n",
    "    exit()\n",
    "\n",
    "def classify_toxicity(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Use toxic-bert to classify if text is toxic\n",
    "    Returns: dict with toxicity classification and confidence\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle empty or very short text\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return {\n",
    "                'is_toxic': False,\n",
    "                'is_safe': True,\n",
    "                'confidence': 0.0,\n",
    "                'label': 'NON_TOXIC'\n",
    "            }\n",
    "        \n",
    "        # Get prediction from toxic-bert\n",
    "        result = toxicity_classifier(text)\n",
    "        \n",
    "        # Handle different response formats\n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "        \n",
    "        is_toxic = result['label'] == 'TOXIC'\n",
    "        confidence = result['score']\n",
    "        \n",
    "        # Apply threshold for final decision\n",
    "        final_toxic = is_toxic and confidence > threshold\n",
    "        \n",
    "        return {\n",
    "            'is_toxic': final_toxic,\n",
    "            'is_safe': not final_toxic,\n",
    "            'confidence': confidence,\n",
    "            'label': result['label'],\n",
    "            'raw_score': result['score']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error classifying text: {e}\")\n",
    "        # Default to safe if there's an error\n",
    "        return {\n",
    "            'is_toxic': False,\n",
    "            'is_safe': True,\n",
    "            'confidence': 0.0,\n",
    "            'label': 'ERROR',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def filter_dataset_with_toxic_bert(dataset_stream, toxicity_threshold=0.7, max_samples=None):\n",
    "    \"\"\"\n",
    "    Filter the ClimbLab dataset using toxic-bert\n",
    "    \n",
    "    Args:\n",
    "        dataset_stream: Your ClimbLab dataset stream\n",
    "        toxicity_threshold: Confidence threshold (0.1-0.9, higher = more strict)\n",
    "        max_samples: Limit samples for testing (None = process all)\n",
    "    \n",
    "    Returns:\n",
    "        safe_data: List of clean samples\n",
    "        toxic_data: List of toxic samples\n",
    "        stats: Processing statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Storage lists\n",
    "    safe_data = []\n",
    "    toxic_data = []\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        'total_processed': 0,\n",
    "        'safe_count': 0,\n",
    "        'toxic_count': 0,\n",
    "        'error_count': 0,\n",
    "        'empty_count': 0,\n",
    "        'avg_confidence_safe': 0.0,\n",
    "        'avg_confidence_toxic': 0.0\n",
    "    }\n",
    "    \n",
    "    confidence_safe_sum = 0.0\n",
    "    confidence_toxic_sum = 0.0\n",
    "    \n",
    "    print(f\"🔍 Starting toxicity filtering with threshold: {toxicity_threshold}\")\n",
    "    print(f\"🎯 Threshold explanation: {toxicity_threshold:.1f} means {int(toxicity_threshold*100)}% confidence required to mark as toxic\")\n",
    "    print(\"📊 Processing samples...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, sample in enumerate(dataset_stream):\n",
    "        text = sample.get(\"text\", \"\").strip()\n",
    "        \n",
    "        # Skip completely empty samples\n",
    "        if not text:\n",
    "            stats['empty_count'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Classify toxicity\n",
    "        toxicity_result = classify_toxicity(text, threshold=toxicity_threshold)\n",
    "        \n",
    "        # Create enhanced sample with toxicity info\n",
    "        enhanced_sample = {\n",
    "            **sample,\n",
    "            'toxicity_analysis': toxicity_result,\n",
    "            'sample_id': i\n",
    "        }\n",
    "        \n",
    "        # Categorize the sample\n",
    "        if toxicity_result['is_safe']:\n",
    "            safe_data.append(enhanced_sample)\n",
    "            stats['safe_count'] += 1\n",
    "            confidence_safe_sum += toxicity_result['confidence']\n",
    "        else:\n",
    "            toxic_data.append(enhanced_sample)\n",
    "            stats['toxic_count'] += 1\n",
    "            confidence_toxic_sum += toxicity_result['confidence']\n",
    "        \n",
    "        if 'error' in toxicity_result:\n",
    "            stats['error_count'] += 1\n",
    "        \n",
    "        stats['total_processed'] += 1\n",
    "        \n",
    "        # Progress updates\n",
    "        if (i + 1) % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i + 1) / elapsed\n",
    "            print(f\"✅ Processed {i + 1:,} samples ({rate:.1f} samples/sec)\")\n",
    "            print(f\"   📈 Safe: {stats['safe_count']:,} | 🚨 Toxic: {stats['toxic_count']:,}\")\n",
    "        \n",
    "        # Optional limit for testing\n",
    "        if max_samples and i >= max_samples - 1:\n",
    "            print(f\"🔄 Stopping at {max_samples} samples (testing mode)\")\n",
    "            break\n",
    "    \n",
    "    # Calculate averages\n",
    "    if stats['safe_count'] > 0:\n",
    "        stats['avg_confidence_safe'] = confidence_safe_sum / stats['safe_count']\n",
    "    if stats['toxic_count'] > 0:\n",
    "        stats['avg_confidence_toxic'] = confidence_toxic_sum / stats['toxic_count']\n",
    "    \n",
    "    # Final timing\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n🏁 FILTERING COMPLETE!\")\n",
    "    print(f\"⏱️  Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"🚀 Average rate: {stats['total_processed']/total_time:.1f} samples/second\")\n",
    "    \n",
    "    return safe_data, toxic_data, stats\n",
    "\n",
    "def print_filtering_results(safe_data, toxic_data, stats):\n",
    "    \"\"\"Print detailed results of the filtering process\"\"\"\n",
    "    \n",
    "    total = stats['safe_count'] + stats['toxic_count']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 FILTERING RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"📝 Total samples processed: {stats['total_processed']:,}\")\n",
    "    print(f\"📋 Empty samples skipped: {stats['empty_count']:,}\")\n",
    "    print(f\"⚠️  Processing errors: {stats['error_count']:,}\")\n",
    "    \n",
    "    print(f\"\\n✅ SAFE CONTENT: {stats['safe_count']:,} samples ({stats['safe_count']/total*100:.1f}%)\")\n",
    "    print(f\"🚨 TOXIC CONTENT: {stats['toxic_count']:,} samples ({stats['toxic_count']/total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n🎯 Average confidence scores:\")\n",
    "    print(f\"   Safe content: {stats['avg_confidence_safe']:.3f}\")\n",
    "    print(f\"   Toxic content: {stats['avg_confidence_toxic']:.3f}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\n📋 SAMPLE TOXIC CONTENT (first 3):\")\n",
    "    for i, sample in enumerate(toxic_data[:3]):\n",
    "        analysis = sample['toxicity_analysis']\n",
    "        text_preview = sample['text'][:150] + \"...\" if len(sample['text']) > 150 else sample['text']\n",
    "        print(f\"\\nToxic Sample {i+1}:\")\n",
    "        print(f\"   Confidence: {analysis['confidence']:.3f}\")\n",
    "        print(f\"   Text: {text_preview}\")\n",
    "    \n",
    "    print(f\"\\n📋 SAMPLE SAFE CONTENT (first 3):\")\n",
    "    for i, sample in enumerate(safe_data[:3]):\n",
    "        analysis = sample['toxicity_analysis']\n",
    "        text_preview = sample['text'][:150] + \"...\" if len(sample['text']) > 150 else sample['text']\n",
    "        print(f\"\\nSafe Sample {i+1}:\")\n",
    "        print(f\"   Confidence: {analysis['confidence']:.3f}\")\n",
    "        print(f\"   Text: {text_preview}\")\n",
    "\n",
    "# 🚀 CUSTOMIZABLE FILTERING SETUP\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 READY TO FILTER YOUR CLIMBLAB DATASET!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 🎛️ CUSTOMIZE THESE SETTINGS:\n",
    "NUM_SAMPLES_TO_PROCESS = 500    # 👈 CHANGE THIS NUMBER!\n",
    "TOXICITY_THRESHOLD = 0.7        # 👈 ADJUST STRICTNESS (0.5-0.9)\n",
    "\n",
    "print(f\"\\n⚙️  CURRENT SETTINGS:\")\n",
    "print(f\"   📊 Samples to process: {NUM_SAMPLES_TO_PROCESS:,}\")\n",
    "print(f\"   🎯 Toxicity threshold: {TOXICITY_THRESHOLD}\")\n",
    "print(f\"   📁 Dataset variable: ds\")\n",
    "\n",
    "print(f\"\\n🚀 STARTING FILTERING...\")\n",
    "\n",
    "# Start filtering with your dataset\n",
    "safe_samples, toxic_samples, filtering_stats = filter_dataset_with_toxic_bert(\n",
    "    ds,                           # Your dataset variable\n",
    "    toxicity_threshold=TOXICITY_THRESHOLD,\n",
    "    max_samples=NUM_SAMPLES_TO_PROCESS\n",
    ")\n",
    "\n",
    "# Print detailed results\n",
    "print_filtering_results(safe_samples, toxic_samples, filtering_stats)\n",
    "\n",
    "print(f\"\\n✅ FILTERING COMPLETE!\")\n",
    "print(f\"📦 Safe samples ready to use: {len(safe_samples)}\")\n",
    "print(f\"🚨 Toxic samples flagged: {len(toxic_samples)}\")\n",
    "print(f\"💾 Both lists are stored in: safe_samples, toxic_samples\")\n",
    "\n",
    "print(f\"\\n🔄 TO PROCESS MORE SAMPLES:\")\n",
    "print(f\"   Just change NUM_SAMPLES_TO_PROCESS = {NUM_SAMPLES_TO_PROCESS} to a higher number\")\n",
    "print(f\"   Then run the cell again!\")\n",
    "\n",
    "print(\"\\n🎛️ THRESHOLD GUIDE:\")\n",
    "print(\"   0.5 = Lenient (catches obvious toxicity)\")\n",
    "print(\"   0.7 = Balanced (recommended)\")\n",
    "print(\"   0.8 = Strict (very cautious)\")\n",
    "print(\"   0.9 = Very strict (might over-filter)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
