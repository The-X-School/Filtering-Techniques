from transformers import pipeline, GPT2Tokenizer, AutoTokenizer
from tqdm import tqdm
import re
from math import ceil

pipe = pipeline("text-generation", model="prometheus-eval/prometheus-7b-v2.0")
gpt2tokenizer = GPT2Tokenizer.from_pretrained("gpt2", use_fast=True)
tokenizer = AutoTokenizer.from_pretrained("prometheus-eval/prometheus-7b-v2.0")

def detokenize(token_ids, tokenizer):
    """Detokenizes a list of token IDs into a cleaned string."""
    if not token_ids:
        return ""
    return tokenizer.decode(token_ids, skip_special_tokens=True).strip()

def truncate_text(text, tokenizer, max_tokens):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
        truncated_text = tokenizer.decode(tokens, clean_up_tokenization_spaces=True)
        return truncated_text + " [...]"  # indicate truncation
    return text

def evaluate(dataset, instruction, ref_answer, criteria, desc, trunc_length, max_length, batch_size=8):
    """
    dataset: list of token ID lists (each item is a list of ints)
    instruction: str, instruction text for the prompt
    ref_answer: str, example of text with score = 5
    criteria: str, short description of evaluation criteria header
    desc: list of 5 strings describing each score level (index 0 is score 1 desc)
    trunc_length: int, max tokens allowed in 'Response to evaluate' text before truncation
    max_length: int, max total tokens generated by model (prompt + generation)
    batch_size: int, number of samples to process in a batch
    """
    scores = []
    num_batches = ceil(len(dataset) / batch_size)

    for i in tqdm(range(num_batches), desc="Evaluating Data (batches)"):
        batch_docs = dataset[i*batch_size : (i+1)*batch_size]
        
        prompts = []
        for doc in batch_docs:
            detoken = detokenize(doc, gpt2tokenizer)
            detoken = truncate_text(detoken, tokenizer, trunc_length)
            
            prompt = (
                "Task Description: An instruction (might include an Input inside it), a response to evaluate, "
                "a reference answer that gets a score of 5, and a score rubric representing evaluation criteria are given.\n"
                "1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general.\n"
                "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n"
                "3. The output format should look as follows: \"[FEEDBACK]: (write a feedback for criteria) [RESULT]: (an integer number between 1 and 5)\"\n"
                "4. Please do not generate any other opening, closing, and explanations.\n\n"
                f"The instruction to evaluate:\n{instruction}\n\n"
                f"Response to evaluate:\n{detoken}\n\n"
                f"Reference Answer (Score 5):\n{ref_answer}\n\n"
                f"Score Rubrics:\n{criteria}\n"
                f"Score 1: {desc[0]}\n"
                f"Score 2: {desc[1]}\n"
                f"Score 3: {desc[2]}\n"
                f"Score 4: {desc[3]}\n"
                f"Score 5: {desc[4]}"
            )
            prompts.append(prompt)

        outputs = pipe(prompts, max_length=max_length, do_sample=False)
        
        for output in outputs:
            generated_text = output['generated_text']
            match = re.search(r'\[RESULT\]:?\s*([1-5])', generated_text)
            score = int(match.group(1)) if match else 3  # fallback score
            scores.append(score)

    return scores
