from transformers import pipeline, GPT2Tokenizer, AutoTokenizer
from datasets import load_dataset, IterableDataset
from tqdm import tqdm
import re
import json
from math import ceil

pipe = pipeline("text-generation", model="prometheus-eval/prometheus-7b-v2.0")
gpt2tokenizer = GPT2Tokenizer.from_pretrained("gpt2", use_fast=True)
tokenizer = AutoTokenizer.from_pretrained("prometheus-eval/prometheus-7b-v2.0")

def detokenize(token_ids, tokenizer):
    """Detokenizes a list of token IDs into a cleaned string."""
    if not token_ids:
        return ""
    return tokenizer.decode(token_ids, skip_special_tokens=True).strip()

def truncate_text(text, tokenizer, max_tokens):
    tokens = tokenizer.encode(text, add_special_tokens=False, truncation=True, max_length=max_tokens)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
        truncated_text = tokenizer.decode(tokens, clean_up_tokenization_spaces=True)
        return truncated_text + " [...]"  # indicate truncation
    return text

def evaluate(dataset, instruction, ref_answer, criteria, desc, trunc_length, max_new_tokens, batch_size=8):
    """
    dataset: list of token ID lists (each item is a list of ints)
    instruction: str, instruction text for the prompt
    ref_answer: str, example of text with score = 5
    criteria: str, short description of evaluation criteria header
    desc: list of 5 strings describing each score level (index 0 is score 1 desc)
    trunc_length: int, max tokens allowed in 'Response to evaluate' text before truncation
    max_new_tokens: int, max new tokens generated by model
    batch_size: int, number of samples to process in a batch
    """

    scores = []
    num_batches = ceil(len(dataset) / batch_size)

    for i in tqdm(range(num_batches), desc="Evaluating Data (batches)"):
        batch_docs = dataset[i*batch_size : (i+1)*batch_size]
        
        prompts = []
        for doc in batch_docs:
            detoken = detokenize(doc, gpt2tokenizer)
            detoken = truncate_text(detoken, tokenizer, trunc_length)
            
            prompt = (
                "Task Description: An instruction (might include an Input inside it), a response to evaluate, "
                "a reference answer that gets a score of 5, and a score rubric representing evaluation criteria are given.\n"
                "1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general.\n"
                "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n"
                "3. The output format should look as follows: \"[FEEDBACK]: (write a feedback for criteria) [RESULT]: (an integer number between 1 and 5)\"\n"
                "4. Please do not generate any other opening, closing, and explanations.\n\n"
                f"The instruction to evaluate:\n{instruction}\n\n"
                f"Response to evaluate:\n{detoken}\n\n"
                f"Reference Answer (Score 5):\n{ref_answer}\n\n"
                f"Score Rubrics:\n{criteria}\n"
                f"Score 1: {desc[0]}\n"
                f"Score 2: {desc[1]}\n"
                f"Score 3: {desc[2]}\n"
                f"Score 4: {desc[3]}\n"
                f"Score 5: {desc[4]}"
            )
            prompts.append(prompt)

        outputs = pipe(prompts, max_new_tokens=max_new_tokens, do_sample=False)
        
        for output, doc in zip(outputs, batch_docs):
            generated_text = output[0]['generated_text']
            match = re.search(r'\[RESULT\]:?\s*([1-5])', generated_text)
            score = int(match.group(1)) if match else 3  # fallback score
            scores.append((score, doc))

            feedback_match = re.search(r'\[FEEDBACK\]:\s*(.*)', generated_text, re.DOTALL)
            if feedback_match:
                feedback = feedback_match.group(1).rstrip()  # trims trailing whitespace
                print("--- FEEDBACK ---\n" + feedback + "\n--- END FEEDBACK ---")
                
    with open('scores.json', 'w') as f:
        json.dump(scores, f)

    return scores

def main():
    total_sample = 10
    print("Step 0.1: Loading dataset...")
    full_dataset = load_dataset("nvidia/ClimbLab", split="train", streaming=True)

    # Convert to list of token lists
    print("Step 0.2: Collecting sample...")
    dataset = []
    for i, item in enumerate(tqdm(full_dataset, desc="Collecting sample", total=total_sample)):
        if i >= total_sample:
            break
        dataset.append(item)

    print("Step 0.3: Reformatting sample...")
    for i, item in enumerate(tqdm(dataset, desc="Reformatting sample")):
        dataset[i] = list(item["tokens"])
    
    print(f"Loaded {len(dataset)} documents")

    instruction = "Evaluate the text based on how well it can improve a language model's roleplaying abilities."
    ref_ans = "[Setting: A dimly lit medieval tavern, crackling fireplace in the corner. Rain hammers the windows. The rogue, Kael, sits alone, hood shadowing his face, nursing a mug of bitter ale. A paladin named Seraphina enters, armor glinting, expression tense.]\n\n" \
         "Seraphina: (striding up, tone clipped) You missed the rendezvous, Kael. Again. Three nights in a row.\n\n" \
         "Kael: (without looking up) And yet, here you are. Safe, armored, and righteously pissed. I call that a win.\n\n" \
         "Seraphina: (slams gauntlet on the table) People *died* waiting for you.\n\n" \
         "Kael: (finally looks up, voice low) People die every day, Seraphina. That's the price of doing *your* kind of good.\n\n" \
         "[She falters, anger simmering beneath her righteous indignation.] [...]"
    criteria = "Roleplaying Quality: Does the response exhibit strong in-character consistency, immersive dialogue, and emotional or narrative depth in line with the given instruction?"
    desc = [
    "The response rarely includes any roleplaying content. Characters and dialogue are mostly absent.",
    "The response occasionally hints at roleplaying, such as brief mentions of characters or setting, but it remains vague or limited.",
    "The response sometimes includes basic roleplaying elements like setting or characters, though dialogue or interaction is minimal.",
    "The response often includes characters and dialogue. Interaction is present, even if the writing isn't very vivid or detailed.",
    "The response frequently includes roleplaying with characters, dialogue, and interaction that contribute to a narrativeâ€”without needing to be highly vivid or elaborate."
    ]
    evaluate(dataset, instruction, ref_ans, criteria, desc, trunc_length=512, max_new_tokens=256, batch_size=8)

if __name__ == "__main__":
    main()
